{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica Final Aprendizaje por refuerzo. Value Iteration vs Temporal Difference Learning vs Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta práctica es comparar los tres algoritmos de aprendizaje que conocemos de aprendizaje por refuerzo en el problema del Taxi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos todas las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "from time import sleep\n",
    "import math\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos como podemos crear un ambiente del problema del Taxi y como podemos visualizarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n   #Número de estados\n",
    "print(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n   #Número de acciones\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero debes programar el algoritmo **Value Iteration** para obtener la estrategia óptima. En el problema del Taxi, la matriz P contiene el modelo de transición de estados `[(p, snew, rew, done)] = env.P[state][a]`. p corresponde a la probabilidad de pasar de **s** a **snew** al aplicar la acción **a**. Todas las probabilidades son 1, por tanto es un modelo de transición de estados determinista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueIteration(env):\n",
    "    n_states = env.observation_space.n\n",
    "    Value = [0 for i in range(n_states)]\n",
    "\n",
    "            \n",
    "    return Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que has obtenido el vector V con las utilidades de los estados, con la siguiente función puedes obtener la estrategia óptima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromVtopolicy(env, V):\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    for s in range(env.observation_space.n):\n",
    "        values = []\n",
    "        for a in range(env.action_space.n):\n",
    "            [(p, s_, rew, done)] = env.P[s][a]\n",
    "            values.append(rew+ gamma*V[s_])\n",
    "        best_act= np.argmax(values)\n",
    "        policy[s,best_act] = 1.0\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "Vopt = ValueIteration(env)\n",
    "policyopt = fromVtopolicy(env, Vopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la siguiente función contamos el número de pasos que cuesta llegar hasta el estado final siguiendo las acciones de una estrategia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(policy, env):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    done=False\n",
    "    while(not done):\n",
    "        state, reward, done, info = env.step(np.argmax(policy[curr_state]))\n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma podemos mostrar el número de pasos que cuesta llegar hasta el estado final para diferentes estados iniciales y calculamos la media:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "curr_state = env.reset()\n",
    "val_counts = [count(policyopt, env) for i in range(1000)]\n",
    "print(\"Un agente que utiliza una estrategia obtenida por Value iteration le cuesta una media de \" + str(np.mean(val_counts))\n",
    "      + \" pasos para completar satisfactoriamente la misión.\")\n",
    "sns.distplot(val_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "En algoritmo Q-learning el agente va a ir realizando episodios (intentos de resolver el problema) y cada vez que realice una acción va a ir actualizando la tabla de valores $Q$. El valor $Q(s,a)$ representa la calidad o utilidad de realizar una acción $a$ estando en un estado $s$. El valor de calidad Q es otra forma de representar la ecuación de Bellman sobre la utilidad esperada de un estado:\n",
    "\n",
    "$Q(s,a) = R(s,a) + \\gamma \\sum_{s'}T(s,a,s')max_{a'}Q(s',a')$ \n",
    "\n",
    "A partir de esta ecuación basandonos en idea de *temporal difference learning* obtenemos la ecuación de actualización de los valores de $Q$:\n",
    "\n",
    "$Q(s,a) := Q(s,a) + \\alpha(R(s,a) + \\gamma max_{a'}{Q(s',a')} - Q(s,a))$\n",
    "\n",
    "Primero vamos a crear la tabla Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que en el periodo de aprendizaje el agente pruebe caminos que no son los que le indica la tabla Q, vamos a hacer que el agente tome acciones aleatorias con una probabilidad. Crea una función que devuelva la acción a realizar según la tabla Q si un valor que generamos de forma aletaria es mayor que epsilon (por defecto epsilon = 0.1) y en caso contrario la acción será aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActionwithExploration(Q, n_actions, s, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Q: tabla de utilidad (n_states x n_actions)\n",
    "    epsilon: parámetro para la exploración\n",
    "    s: estado actual\n",
    "    \"\"\"\n",
    "    rand=np.random.uniform(0,1)\n",
    "\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los valores de los parámetros para converger a una solución. Primero prueba tu algoritmo con menos episodios para comprobar que converge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "episodes = 10000 # 100\n",
    "max_steps = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completa el código del algoritmo Q_learning siguiendo el pseudocogido de las transparencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "timestep_reward = []\n",
    "\n",
    "for episode in range(episodes):\n",
    " \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    t = 0\n",
    "    while t < max_steps:\n",
    "        t += 1\n",
    "        # Completa aquí tu código\n",
    "       \n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            if episode % 10 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Este episodio {episode} tuvo {t} pasos y recompensa de {total_reward}\")\n",
    "            timestep_reward.append(total_reward)\n",
    "            break\n",
    "\n",
    "        \n",
    "print(\"Entrenamiento de la tabla Q ha finalizado.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el vector timestep_reward para ver si el algoritmo converge. Si la recomensa total del episodio cada vez es menor significa que converge ya que los valores de los estados guían al taxi a realizar acciones correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(timestep_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea una función fromQtopolicy(env,Q) en la que se devuelva la estrategia en forma matrix (n_estados x n_acciones) a partir de los valores de la utilizadad Q(s,a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromQtopolicy(env,Q):\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comprobar el número medio de pasos obtenidos por la estrategia aprendida por Q-Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "Q_learn_pol = fromQtopolicy(env,Q_table)\n",
    "Q_counts = [count(Q_learn_pol,env) for i in range(10000)]\n",
    "print(\"Un agente que utiliza una estrategia obtenida por Q-learning le cuesta una media de \" + str(np.mean(val_counts))\n",
    "      + \" pasos para completar satisfactoriamente la misión.\")\n",
    "sns.distplot(Q_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante:** Compara las estrategias obtenidas con el algoritmo Value Iteration y con Q-Learning. ¿La media de pasos hasta el estado final es la misma?,¿En cuantos estados la acción escogida es diferente? Realiza algunas mejoras al algoritmo Q-Learning para ayudar a que converja a la estrategia optima. Por ejemplo, puedes crear una función Potencial (con `(x,y,p,d)=env.decode(estado)` puedes obtener la posición x, y del taxi, donde está el pasajero p y el destino d), también  puedes modificar los valores de alpha y epsilón confome avanzan los episodios. \n",
    "Añade las celdas que consideres oportuno y una celda con texto al final para explicar lo que has hecho y las conclusiones que has obtenido.\n",
    "\n",
    "**Extra:** Realiza la misma comparación y propón alguna mejora con el algoritmo TDL(lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:** Con esta función podemos visualizar el recorrido del taxi siguiendo las acciones de la estrategia: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "curr_state = env.reset()\n",
    "\n",
    "estrategia = Q_learn_pol\n",
    "done = False\n",
    "frames = []\n",
    "while not done:\n",
    "    a = np.argmax(estrategia[curr_state])\n",
    "    state, reward, done, info = env.step(a) \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    curr_state = state\n",
    "    \n",
    "print_frames(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
