{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica Final Aprendizaje por refuerzo. Value Iteration vs Temporal Difference Learning vs Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta práctica es comparar los tres algoritmos de aprendizaje que conocemos de aprendizaje por refuerzo en el problema del Taxi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos todas las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arribas\\anaconda3\\envs\\TFG\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "c:\\Users\\Arribas\\anaconda3\\envs\\TFG\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "from time import sleep\n",
    "import math\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos como podemos crear un ambiente del problema del Taxi y como podemos visualizarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n   #Número de estados\n",
    "print(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n   #Número de acciones\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero debes programar el algoritmo **Value Iteration** para obtener la estrategia óptima. En el problema del Taxi, la matriz P contiene el modelo de transición de estados `[(p, snew, rew, done)] = env.P[state][a]`. p corresponde a la probabilidad de pasar de **s** a **snew** al aplicar la acción **a**. Todas las probabilidades son 1, por tanto es un modelo de transición de estados determinista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueIteration(env):\n",
    "    n_states = env.observation_space.n\n",
    "    Value = [0 for i in range(n_states)]\n",
    "\n",
    "            \n",
    "    return Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que has obtenido el vector V con las utilidades de los estados, con la siguiente función puedes obtener la estrategia óptima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromVtopolicy(env, V):\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    for s in range(env.observation_space.n):\n",
    "        values = []\n",
    "        for a in range(env.action_space.n):\n",
    "            [(p, s_, rew, done)] = env.P[s][a]\n",
    "            values.append(rew+ gamma*V[s_])\n",
    "        best_act= np.argmax(values)\n",
    "        policy[s,best_act] = 1.0\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "Vopt = ValueIteration(env)\n",
    "policyopt = fromVtopolicy(env, Vopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la siguiente función contamos el número de pasos que cuesta llegar hasta el estado final siguiendo las acciones de una estrategia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(policy, env):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    done=False\n",
    "    while(not done):\n",
    "        state, reward, done, info = env.step(np.argmax(policy[curr_state]))\n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma podemos mostrar el número de pasos que cuesta llegar hasta el estado final para diferentes estados iniciales y calculamos la media:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un agente que utiliza una estrategia obtenida por Value iteration le cuesta una media de 200.0 pasos para completar satisfactoriamente la misión.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "curr_state = env.reset()\n",
    "val_counts = [count(policyopt, env) for i in range(1000)]\n",
    "print(\"Un agente que utiliza una estrategia obtenida por Value iteration le cuesta una media de \" + str(np.mean(val_counts))\n",
    "      + \" pasos para completar satisfactoriamente la misión.\")\n",
    "sns.distplot(val_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "En algoritmo Q-learning el agente va a ir realizando episodios (intentos de resolver el problema) y cada vez que realice una acción va a ir actualizando la tabla de valores $Q$. El valor $Q(s,a)$ representa la calidad o utilidad de realizar una acción $a$ estando en un estado $s$. El valor de calidad Q es otra forma de representar la ecuación de Bellman sobre la utilidad esperada de un estado:\n",
    "\n",
    "$Q(s,a) = R(s,a) + \\gamma \\sum_{s'}T(s,a,s')max_{a'}Q(s',a')$ \n",
    "\n",
    "A partir de esta ecuación basandonos en idea de *temporal difference learning* obtenemos la ecuación de actualización de los valores de $Q$:\n",
    "\n",
    "$Q(s,a) := Q(s,a) + \\alpha(R(s,a) + \\gamma max_{a'}{Q(s',a')} - Q(s,a))$\n",
    "\n",
    "Primero vamos a crear la tabla Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que en el periodo de aprendizaje el agente pruebe caminos que no son los que le indica la tabla Q, vamos a hacer que el agente tome acciones aleatorias con una probabilidad. Crea una función que devuelva la acción a realizar según la tabla Q si un valor que generamos de forma aletaria es mayor que epsilon (por defecto epsilon = 0.1) y en caso contrario la acción será aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActionwithExploration(Q, n_actions, s, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Q: tabla de utilidad (n_states x n_actions)\n",
    "    epsilon: parámetro para la exploración\n",
    "    s: estado actual\n",
    "    \"\"\"\n",
    "    rand=np.random.uniform(0,1)\n",
    "\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los valores de los parámetros para converger a una solución. Primero prueba tu algoritmo con menos episodios para comprobar que converge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "episodes = 10000 # 100\n",
    "max_steps = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completa el código del algoritmo Q_learning siguiendo el pseudocogido de las transparencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento de la tabla Q ha finalizado.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "timestep_reward = []\n",
    "\n",
    "for episode in range(episodes):\n",
    " \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    t = 0\n",
    "    while t < max_steps:\n",
    "        t += 1\n",
    "        # Completa aquí tu código\n",
    "       \n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            if episode % 10 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Este episodio {episode} tuvo {t} pasos y recompensa de {total_reward}\")\n",
    "            timestep_reward.append(total_reward)\n",
    "            break\n",
    "\n",
    "        \n",
    "print(\"Entrenamiento de la tabla Q ha finalizado.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el vector timestep_reward para ver si el algoritmo converge. Si la recomensa total del episodio cada vez es menor significa que converge ya que los valores de los estados guían al taxi a realizar acciones correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c5ef6ba820>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(timestep_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea una función fromQtopolicy(env,Q) en la que se devuelva la estrategia en forma matrix (n_estados x n_acciones) a partir de los valores de la utilizadad Q(s,a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromQtopolicy(env,Q):\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comprobar el número medio de pasos obtenidos por la estrategia aprendida por Q-Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un agente que utiliza una estrategia obtenida por Q-learning le cuesta una media de 200.0 pasos para completar satisfactoriamente la misión.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "Q_learn_pol = fromQtopolicy(env,Q_table)\n",
    "Q_counts = [count(Q_learn_pol,env) for i in range(10000)]\n",
    "print(\"Un agente que utiliza una estrategia obtenida por Q-learning le cuesta una media de \" + str(np.mean(val_counts))\n",
    "      + \" pasos para completar satisfactoriamente la misión.\")\n",
    "sns.distplot(Q_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante:** Compara las estrategias obtenidas con el algoritmo Value Iteration y con Q-Learning. ¿La media de pasos hasta el estado final es la misma?,¿En cuantos estados la acción escogida es diferente? Realiza algunas mejoras al algoritmo Q-Learning para ayudar a que converja a la estrategia optima. Por ejemplo, puedes crear una función Potencial (con `(x,y,p,d)=env.decode(estado)` puedes obtener la posición x, y del taxi, donde está el pasajero p y el destino d), también  puedes modificar los valores de alpha y epsilón confome avanzan los episodios. \n",
    "Añade las celdas que consideres oportuno y una celda con texto al final para explicar lo que has hecho y las conclusiones que has obtenido.\n",
    "\n",
    "**Extra:** Realiza la misma comparación y propón alguna mejora con el algoritmo TDL(lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:** Con esta función podemos visualizar el recorrido del taxi siguiendo las acciones de la estrategia: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Arribas\\Desktop\\Code\\TFG_Atari_NeuralNetworks\\PracticaFinalAR.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=8'>9</a>\u001b[0m     a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(estrategia[curr_state])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=9'>10</a>\u001b[0m     state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(a) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=10'>11</a>\u001b[0m     frames\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=11'>12</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m: env\u001b[39m.\u001b[39mrender(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mansi\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=12'>13</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m: state,\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=13'>14</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m: action,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=14'>15</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m'\u001b[39m: reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=15'>16</a>\u001b[0m         }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=16'>17</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=17'>18</a>\u001b[0m     curr_state \u001b[39m=\u001b[39m state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arribas/Desktop/Code/TFG_Atari_NeuralNetworks/PracticaFinalAR.ipynb#ch0000034?line=19'>20</a>\u001b[0m print_frames(frames)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'action' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import random \n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "curr_state = env.reset()\n",
    "\n",
    "estrategia = Q_learn_pol\n",
    "done = False\n",
    "frames = []\n",
    "while not done:\n",
    "    a = np.argmax(estrategia[curr_state])\n",
    "    state, reward, done, info = env.step(a) \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    curr_state = state\n",
    "    \n",
    "print_frames(frames)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e05437fa0fc53250a4e4becd1344dfe15cafc8b3911f20ffda9bbc92cb7c9efc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('TFG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
