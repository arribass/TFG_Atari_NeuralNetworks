training:
  batch_size: 32
  learning_rate: 0.001
  loss: huber
  num_episodes: 10000
  train_steps: 1000000
  warmup_episodes: 1000
  save_freq: 1000

optimizer:
  name: adam
  lr_min: 0.0001
  lr_decay: 5000

rl:
  gamma: 0.99
  max_steps_per_episode: 7000
  target_model_update_episodes: 10
  memory_size: 50000

epsilon:
  max_epsilon: 1
  min_epsilon: 0.1
  decay: 400

telegram:
  msg_freq: 200
  graph_freq: 1000